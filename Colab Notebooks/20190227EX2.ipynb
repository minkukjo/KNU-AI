{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20190227EX2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"uNME0eeB4J7c","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras.datasets.cifar100 import load_data"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RtrBbSra4Wj0","colab_type":"code","colab":{}},"cell_type":"code","source":["(x_train,y_train),(x_test,y_test) = load_data()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fWqNuzpE5nJ7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d7384256-d1f6-4502-ebb1-9fd87b85370f","executionInfo":{"status":"ok","timestamp":1551254929787,"user_tz":-540,"elapsed":1941,"user":{"displayName":"조민국","photoUrl":"","userId":"08903633611330981973"}}},"cell_type":"code","source":["print(x_train.max())"],"execution_count":41,"outputs":[{"output_type":"stream","text":["255\n"],"name":"stdout"}]},{"metadata":{"id":"DJwG6nQL5uWV","colab_type":"code","colab":{}},"cell_type":"code","source":["x_train = x_train/255\n","x_test = x_test/255"],"execution_count":0,"outputs":[]},{"metadata":{"id":"77doPis35xM0","colab_type":"code","colab":{}},"cell_type":"code","source":["y_train = y_train.reshape([-1])\n","y_test = y_test.reshape([-1])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UV2b-2SS56YE","colab_type":"code","colab":{}},"cell_type":"code","source":["x = tf.placeholder(tf.float32, shape=[None,32,32,3])\n","y = tf.placeholder(tf.int64, shape=[None])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DniuBSKU6AL3","colab_type":"code","colab":{}},"cell_type":"code","source":["y_onehot = tf.one_hot(y,100)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2DvcTuHO6Bol","colab_type":"code","colab":{}},"cell_type":"code","source":["keep_prob = tf.placeholder(tf.float32)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lT11RGfZ6FKQ","colab_type":"code","colab":{}},"cell_type":"code","source":["W_conv1 = tf.Variable(tf.truncated_normal(shape=[5,5,3,64], stddev = 5e-2))\n","b_conv1 = tf.Variable(tf.constant(0.1, shape = [64]))\n","h_conv1 = tf.nn.relu(tf.nn.conv2d(x,W_conv1,strides=[1,1,1,1],padding='SAME')+b_conv1)\n","h_pool1 = tf.nn.max_pool(h_conv1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wvbK-Z-W6niv","colab_type":"code","colab":{}},"cell_type":"code","source":["W_conv2 = tf.Variable(tf.truncated_normal(shape=[5,5,64,64], stddev= 5e-2))\n","b_conv2 = tf.Variable(tf.constant(0.1, shape=[64]))\n","h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1,W_conv2,strides=[1,1,1,1],padding='SAME')+b_conv2)\n","h_pool2 = tf.nn.max_pool(h_conv2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"h3ipRt_z70Tp","colab_type":"code","colab":{}},"cell_type":"code","source":["W_fc1 = tf.Variable(tf.truncated_normal(shape=[8*8*64,384],stddev=5e-2))\n","b_fc1 = tf.Variable(tf.constant(0.1,shape=[384]))\n","h_pool2_flat = tf.reshape(h_pool2,[-1,8*8*64])\n","h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1)+b_fc1)\n","h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob = keep_prob)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vf_c9TDE8P6o","colab_type":"code","colab":{}},"cell_type":"code","source":["W_fc2 = tf.Variable(tf.truncated_normal(shape=[384,100],stddev=5e-2))\n","b_fc2 = tf.Variable(tf.constant(0.1,shape=[100]))\n","logits = tf.matmul(h_fc1_drop,W_fc2)+b_fc2\n","y_pred = tf.nn.softmax(logits)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Uz_4fSNP8jGY","colab_type":"code","colab":{}},"cell_type":"code","source":["loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_onehot,logits=logits))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TrGaQAdx8q-S","colab_type":"code","colab":{}},"cell_type":"code","source":["train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZeuEMJDm8x7V","colab_type":"code","colab":{}},"cell_type":"code","source":["correct_prediction = tf.equal(tf.argmax(y_pred,1),y)\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wv9oRZLj-8EH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1734},"outputId":"62801276-597e-4acd-b49f-f5d28ac1cfeb","executionInfo":{"status":"ok","timestamp":1551255144405,"user_tz":-540,"elapsed":207190,"user":{"displayName":"조민국","photoUrl":"","userId":"08903633611330981973"}}},"cell_type":"code","source":["with tf.Session() as sess:\n","  sess.run(tf.global_variables_initializer())\n","  for i in range(10000):\n","    idx = np.random.randint(x_train.shape[0],size=128)\n","    batch = (x_train[idx],y_train[idx])\n","    if i% 100 == 0:\n","      train_accuracy = sess.run(accuracy,feed_dict={x:batch[0],y:batch[1],keep_prob:1.})\n","      loss_print = loss.eval(feed_dict={x:batch[0],y:batch[1],keep_prob:1.})\n","      print(\"step : %d, acc : %f, loss : %f\" % (i,train_accuracy,loss_print))\n","    sess.run(train_step,feed_dict={x:batch[0],y:batch[1],keep_prob:0.5})  \n","  test_accuracy= accuracy.eval(feed_dict={x:x_test,y:y_test,keep_prob:1.})\n","  print(\"test acc: %f\" % test_accuracy)"],"execution_count":55,"outputs":[{"output_type":"stream","text":["step : 0, acc : 0.023438, loss : 4.850376\n","step : 100, acc : 0.023438, loss : 4.489129\n","step : 200, acc : 0.046875, loss : 4.305403\n","step : 300, acc : 0.132812, loss : 4.034538\n","step : 400, acc : 0.085938, loss : 4.083412\n","step : 500, acc : 0.109375, loss : 3.943452\n","step : 600, acc : 0.250000, loss : 3.702430\n","step : 700, acc : 0.179688, loss : 3.634262\n","step : 800, acc : 0.140625, loss : 3.723573\n","step : 900, acc : 0.148438, loss : 3.648715\n","step : 1000, acc : 0.210938, loss : 3.457349\n","step : 1100, acc : 0.281250, loss : 3.380962\n","step : 1200, acc : 0.234375, loss : 3.317125\n","step : 1300, acc : 0.187500, loss : 3.435218\n","step : 1400, acc : 0.218750, loss : 3.435139\n","step : 1500, acc : 0.203125, loss : 3.392994\n","step : 1600, acc : 0.218750, loss : 3.406193\n","step : 1700, acc : 0.210938, loss : 3.175198\n","step : 1800, acc : 0.242188, loss : 3.177398\n","step : 1900, acc : 0.257812, loss : 3.093365\n","step : 2000, acc : 0.179688, loss : 3.240690\n","step : 2100, acc : 0.312500, loss : 3.105603\n","step : 2200, acc : 0.273438, loss : 3.040370\n","step : 2300, acc : 0.203125, loss : 3.108921\n","step : 2400, acc : 0.218750, loss : 3.194906\n","step : 2500, acc : 0.234375, loss : 3.206038\n","step : 2600, acc : 0.289062, loss : 3.081423\n","step : 2700, acc : 0.335938, loss : 2.918024\n","step : 2800, acc : 0.250000, loss : 3.297831\n","step : 2900, acc : 0.343750, loss : 2.812990\n","step : 3000, acc : 0.281250, loss : 3.040867\n","step : 3100, acc : 0.335938, loss : 2.765962\n","step : 3200, acc : 0.335938, loss : 2.794259\n","step : 3300, acc : 0.281250, loss : 2.895844\n","step : 3400, acc : 0.304688, loss : 2.844052\n","step : 3500, acc : 0.289062, loss : 2.859101\n","step : 3600, acc : 0.281250, loss : 2.915492\n","step : 3700, acc : 0.359375, loss : 2.832856\n","step : 3800, acc : 0.265625, loss : 2.901498\n","step : 3900, acc : 0.312500, loss : 2.665998\n","step : 4000, acc : 0.273438, loss : 2.921155\n","step : 4100, acc : 0.257812, loss : 2.851133\n","step : 4200, acc : 0.312500, loss : 2.838938\n","step : 4300, acc : 0.343750, loss : 2.610481\n","step : 4400, acc : 0.281250, loss : 2.997658\n","step : 4500, acc : 0.312500, loss : 2.825374\n","step : 4600, acc : 0.296875, loss : 2.784772\n","step : 4700, acc : 0.375000, loss : 2.572176\n","step : 4800, acc : 0.328125, loss : 2.717796\n","step : 4900, acc : 0.289062, loss : 2.727645\n","step : 5000, acc : 0.367188, loss : 2.710832\n","step : 5100, acc : 0.398438, loss : 2.448464\n","step : 5200, acc : 0.351562, loss : 2.745268\n","step : 5300, acc : 0.359375, loss : 2.572249\n","step : 5400, acc : 0.390625, loss : 2.655071\n","step : 5500, acc : 0.351562, loss : 2.650116\n","step : 5600, acc : 0.406250, loss : 2.422982\n","step : 5700, acc : 0.429688, loss : 2.413412\n","step : 5800, acc : 0.500000, loss : 2.234651\n","step : 5900, acc : 0.289062, loss : 2.821741\n","step : 6000, acc : 0.359375, loss : 2.536721\n","step : 6100, acc : 0.382812, loss : 2.341967\n","step : 6200, acc : 0.453125, loss : 2.181582\n","step : 6300, acc : 0.421875, loss : 2.305465\n","step : 6400, acc : 0.398438, loss : 2.389067\n","step : 6500, acc : 0.445312, loss : 2.258816\n","step : 6600, acc : 0.437500, loss : 2.247460\n","step : 6700, acc : 0.390625, loss : 2.571144\n","step : 6800, acc : 0.406250, loss : 2.390980\n","step : 6900, acc : 0.500000, loss : 2.090871\n","step : 7000, acc : 0.460938, loss : 2.264656\n","step : 7100, acc : 0.445312, loss : 2.303174\n","step : 7200, acc : 0.453125, loss : 2.297395\n","step : 7300, acc : 0.445312, loss : 2.267874\n","step : 7400, acc : 0.445312, loss : 2.145626\n","step : 7500, acc : 0.476562, loss : 2.040464\n","step : 7600, acc : 0.375000, loss : 2.340477\n","step : 7700, acc : 0.460938, loss : 2.091131\n","step : 7800, acc : 0.414062, loss : 2.369879\n","step : 7900, acc : 0.460938, loss : 2.341728\n","step : 8000, acc : 0.484375, loss : 2.141163\n","step : 8100, acc : 0.507812, loss : 2.182522\n","step : 8200, acc : 0.429688, loss : 2.138061\n","step : 8300, acc : 0.367188, loss : 2.393467\n","step : 8400, acc : 0.468750, loss : 2.167557\n","step : 8500, acc : 0.484375, loss : 2.136127\n","step : 8600, acc : 0.468750, loss : 2.176432\n","step : 8700, acc : 0.531250, loss : 2.100939\n","step : 8800, acc : 0.421875, loss : 2.233380\n","step : 8900, acc : 0.414062, loss : 2.285971\n","step : 9000, acc : 0.531250, loss : 2.047624\n","step : 9100, acc : 0.375000, loss : 2.289048\n","step : 9200, acc : 0.546875, loss : 1.990847\n","step : 9300, acc : 0.437500, loss : 2.175354\n","step : 9400, acc : 0.421875, loss : 2.244020\n","step : 9500, acc : 0.531250, loss : 2.120472\n","step : 9600, acc : 0.429688, loss : 2.216409\n","step : 9700, acc : 0.468750, loss : 1.949870\n","step : 9800, acc : 0.539062, loss : 1.928514\n","step : 9900, acc : 0.515625, loss : 1.991850\n","test acc: 0.373000\n"],"name":"stdout"}]},{"metadata":{"id":"QCDibrVr_lM1","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}