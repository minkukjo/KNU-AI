{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20190227.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"6a5W2tPHb5BR","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras.datasets.mnist import load_data"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pjzrm9jCduwx","colab_type":"code","colab":{}},"cell_type":"code","source":["(x_train,y_train),(x_test,y_test) = load_data()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"i8nH9z_Bd1PK","colab_type":"code","colab":{}},"cell_type":"code","source":["x_train = np.expand_dims(x_train,axis=-1)\n","x_test = np.expand_dims(x_test,axis=-1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BNFnP-jseB6F","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"753cfe74-11d0-42b4-c590-0bb4fe6abf5d","executionInfo":{"status":"ok","timestamp":1551247827262,"user_tz":-540,"elapsed":2764,"user":{"displayName":"조민국","photoUrl":"","userId":"08903633611330981973"}}},"cell_type":"code","source":["print(x_train.max())\n","x_train = x_train/255\n","x_test = x_test/255"],"execution_count":81,"outputs":[{"output_type":"stream","text":["255\n"],"name":"stdout"}]},{"metadata":{"id":"DU4oDNpweSKq","colab_type":"code","colab":{}},"cell_type":"code","source":["x = tf.placeholder(tf.float32,shape=[None,28,28,1])\n","y = tf.placeholder(tf.int64,shape=[None])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7OC_e365e0YR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"577a7730-df6a-4bf9-f67e-825e880d50f9","executionInfo":{"status":"ok","timestamp":1551247827270,"user_tz":-540,"elapsed":2743,"user":{"displayName":"조민국","photoUrl":"","userId":"08903633611330981973"}}},"cell_type":"code","source":["y_onehot = tf.one_hot(y,10)\n","print(y_onehot)"],"execution_count":83,"outputs":[{"output_type":"stream","text":["Tensor(\"one_hot_4:0\", shape=(?, 10), dtype=float32)\n"],"name":"stdout"}]},{"metadata":{"id":"IECuphZqe4VB","colab_type":"code","colab":{}},"cell_type":"code","source":["keep_prob = tf.placeholder(tf.float32)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fkITkvVyf9nG","colab_type":"code","colab":{}},"cell_type":"code","source":["W_conv1 = tf.Variable(tf.truncated_normal(shape=[5, 5, 1, 64], stddev=5e-2))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vSfSI3UhgH2W","colab_type":"code","colab":{}},"cell_type":"code","source":["b_conv1 = tf.Variable(tf.constant(0.1, shape=[64]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HFc4VWl1gQ07","colab_type":"code","colab":{}},"cell_type":"code","source":["# W_conv1 5,5 필터와 x라는 이미지를 strides 1로 함. 텐서는 기본 4차원이기때문에 스트라이드가 4차원이며 기본적으로 중앙 값2개만 사용함.\n","# 활성화 함수로 relu를 사용하고, 1차원 64 bias를 더해줌.\n","h_conv1 = tf.nn.relu(tf.nn.conv2d(x, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2pFeN1XPggLt","colab_type":"code","colab":{}},"cell_type":"code","source":["h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bcr7TCk4hBge","colab_type":"code","colab":{}},"cell_type":"code","source":["# 5,5,64,64인 이유는 이전 W_conv1이 5,5,1,64이기 때문.\n","W_conv2 = tf.Variable(tf.truncated_normal(shape=[5, 5, 64, 64], stddev=5e-2))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Q5sPXI_IhC-D","colab_type":"code","colab":{}},"cell_type":"code","source":["b_conv2 = tf.Variable(tf.constant(0.1, shape=[64]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"42wnEXBehE7_","colab_type":"code","colab":{}},"cell_type":"code","source":["h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ugiObcPnhSyw","colab_type":"code","colab":{}},"cell_type":"code","source":["h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eIeUybMdhUjL","colab_type":"code","colab":{}},"cell_type":"code","source":["# 여기가 7인 이유는, 처음에 x placeholder가 28x28이기때문에, 28에서 풀링 한번에 14, 한번 더하면 7이 되고 그 후 1차원 벡터로 만들어주는 작업이기 때문임.\n","W_fc1 = tf.Variable(tf.truncated_normal(shape=[7 * 7 * 64, 384], stddev=5e-2))\n","b_fc1 = tf.Variable(tf.constant(0.1, shape=[384]))\n","h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n","h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n","h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob=keep_prob)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qzpCzwq0id_S","colab_type":"code","colab":{}},"cell_type":"code","source":["W_fc2 = tf.Variable(tf.truncated_normal(shape=[384, 10], stddev=5e-2))\n","b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]))\n","logits = tf.matmul(h_fc1_drop,W_fc2) + b_fc2\n","y_pred = tf.nn.softmax(logits)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cAZpsSWQijIn","colab_type":"code","colab":{}},"cell_type":"code","source":["loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_onehot, logits=logits))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Apo4kbjAjNfx","colab_type":"code","colab":{}},"cell_type":"code","source":["train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"C4JLEfkDjZl6","colab_type":"code","colab":{}},"cell_type":"code","source":["correct_prediction = tf.equal(tf.argmax(y_pred, 1), y)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NBe09fNdjat2","colab_type":"code","colab":{}},"cell_type":"code","source":["accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TiLWkOuPjbdA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1734},"outputId":"922dfd5c-a3ff-4a7f-9a1c-836baff9202a","executionInfo":{"status":"ok","timestamp":1551247986173,"user_tz":-540,"elapsed":161421,"user":{"displayName":"조민국","photoUrl":"","userId":"08903633611330981973"}}},"cell_type":"code","source":["with tf.Session() as sess:\n","  sess.run(tf.global_variables_initializer())\n","  for i in range(10000):\n","    idx = np.random.randint(x_train.shape[0], size=128)\n","    batch = (x_train[idx], y_train[idx])\n","    if i % 100 == 0:\n","      train_accuracy = sess.run(accuracy, feed_dict= {x: batch[0], y: batch[1], keep_prob: 1.})\n","      loss_print = loss.eval(feed_dict={x: batch[0], y: batch[1], keep_prob: 1.})\n","      print(\"step: %d, acc: %f, loss: %f\" % (i, train_accuracy, loss_print))\n","    sess.run(train_step, feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5})\n","  test_accuracy = accuracy.eval(feed_dict={x: x_test, y: y_test, keep_prob: 1.})\n","  print(\"test acc: %f\" % test_accuracy)"],"execution_count":99,"outputs":[{"output_type":"stream","text":["step: 0, acc: 0.125000, loss: 2.324997\n","step: 100, acc: 0.898438, loss: 0.653130\n","step: 200, acc: 0.898438, loss: 0.359547\n","step: 300, acc: 0.890625, loss: 0.302674\n","step: 400, acc: 0.945312, loss: 0.187661\n","step: 500, acc: 0.945312, loss: 0.260525\n","step: 600, acc: 0.953125, loss: 0.141144\n","step: 700, acc: 0.976562, loss: 0.073207\n","step: 800, acc: 0.976562, loss: 0.162710\n","step: 900, acc: 0.937500, loss: 0.230690\n","step: 1000, acc: 0.968750, loss: 0.149236\n","step: 1100, acc: 0.992188, loss: 0.042140\n","step: 1200, acc: 0.960938, loss: 0.118689\n","step: 1300, acc: 0.984375, loss: 0.043819\n","step: 1400, acc: 0.968750, loss: 0.058135\n","step: 1500, acc: 0.984375, loss: 0.051105\n","step: 1600, acc: 0.960938, loss: 0.123870\n","step: 1700, acc: 0.953125, loss: 0.104512\n","step: 1800, acc: 1.000000, loss: 0.021029\n","step: 1900, acc: 0.976562, loss: 0.060067\n","step: 2000, acc: 1.000000, loss: 0.021355\n","step: 2100, acc: 0.960938, loss: 0.109562\n","step: 2200, acc: 1.000000, loss: 0.024127\n","step: 2300, acc: 0.984375, loss: 0.063855\n","step: 2400, acc: 0.984375, loss: 0.044019\n","step: 2500, acc: 0.992188, loss: 0.023835\n","step: 2600, acc: 0.968750, loss: 0.074651\n","step: 2700, acc: 0.968750, loss: 0.116082\n","step: 2800, acc: 0.984375, loss: 0.039719\n","step: 2900, acc: 0.992188, loss: 0.026813\n","step: 3000, acc: 0.984375, loss: 0.058965\n","step: 3100, acc: 0.976562, loss: 0.050841\n","step: 3200, acc: 0.992188, loss: 0.019576\n","step: 3300, acc: 0.992188, loss: 0.027962\n","step: 3400, acc: 1.000000, loss: 0.011023\n","step: 3500, acc: 0.992188, loss: 0.027951\n","step: 3600, acc: 0.984375, loss: 0.036098\n","step: 3700, acc: 0.984375, loss: 0.040587\n","step: 3800, acc: 0.992188, loss: 0.023809\n","step: 3900, acc: 0.992188, loss: 0.032163\n","step: 4000, acc: 0.992188, loss: 0.017618\n","step: 4100, acc: 0.992188, loss: 0.032892\n","step: 4200, acc: 1.000000, loss: 0.008642\n","step: 4300, acc: 1.000000, loss: 0.004019\n","step: 4400, acc: 0.992188, loss: 0.049186\n","step: 4500, acc: 0.984375, loss: 0.030257\n","step: 4600, acc: 0.992188, loss: 0.059356\n","step: 4700, acc: 0.976562, loss: 0.052843\n","step: 4800, acc: 0.984375, loss: 0.029167\n","step: 4900, acc: 0.992188, loss: 0.017675\n","step: 5000, acc: 0.992188, loss: 0.014626\n","step: 5100, acc: 1.000000, loss: 0.019325\n","step: 5200, acc: 1.000000, loss: 0.004620\n","step: 5300, acc: 0.976562, loss: 0.043625\n","step: 5400, acc: 0.992188, loss: 0.020051\n","step: 5500, acc: 0.976562, loss: 0.082583\n","step: 5600, acc: 1.000000, loss: 0.023259\n","step: 5700, acc: 1.000000, loss: 0.004892\n","step: 5800, acc: 0.992188, loss: 0.009021\n","step: 5900, acc: 1.000000, loss: 0.011879\n","step: 6000, acc: 0.992188, loss: 0.021594\n","step: 6100, acc: 1.000000, loss: 0.007802\n","step: 6200, acc: 1.000000, loss: 0.004746\n","step: 6300, acc: 0.992188, loss: 0.018017\n","step: 6400, acc: 0.984375, loss: 0.037549\n","step: 6500, acc: 1.000000, loss: 0.008487\n","step: 6600, acc: 0.968750, loss: 0.060198\n","step: 6700, acc: 1.000000, loss: 0.011636\n","step: 6800, acc: 1.000000, loss: 0.004743\n","step: 6900, acc: 0.984375, loss: 0.053937\n","step: 7000, acc: 1.000000, loss: 0.002290\n","step: 7100, acc: 0.984375, loss: 0.031186\n","step: 7200, acc: 1.000000, loss: 0.006538\n","step: 7300, acc: 0.992188, loss: 0.032791\n","step: 7400, acc: 1.000000, loss: 0.005974\n","step: 7500, acc: 0.992188, loss: 0.023548\n","step: 7600, acc: 0.992188, loss: 0.018679\n","step: 7700, acc: 0.992188, loss: 0.023011\n","step: 7800, acc: 0.984375, loss: 0.018046\n","step: 7900, acc: 0.992188, loss: 0.013226\n","step: 8000, acc: 1.000000, loss: 0.003177\n","step: 8100, acc: 0.976562, loss: 0.043181\n","step: 8200, acc: 0.992188, loss: 0.014546\n","step: 8300, acc: 1.000000, loss: 0.001962\n","step: 8400, acc: 0.992188, loss: 0.013222\n","step: 8500, acc: 0.992188, loss: 0.014041\n","step: 8600, acc: 0.992188, loss: 0.008528\n","step: 8700, acc: 0.992188, loss: 0.019982\n","step: 8800, acc: 1.000000, loss: 0.000515\n","step: 8900, acc: 0.992188, loss: 0.027416\n","step: 9000, acc: 0.992188, loss: 0.021118\n","step: 9100, acc: 1.000000, loss: 0.006193\n","step: 9200, acc: 1.000000, loss: 0.000883\n","step: 9300, acc: 0.992188, loss: 0.041114\n","step: 9400, acc: 0.992188, loss: 0.022064\n","step: 9500, acc: 1.000000, loss: 0.010909\n","step: 9600, acc: 0.992188, loss: 0.009932\n","step: 9700, acc: 1.000000, loss: 0.001578\n","step: 9800, acc: 1.000000, loss: 0.001967\n","step: 9900, acc: 0.992188, loss: 0.017314\n","test acc: 0.992600\n"],"name":"stdout"}]},{"metadata":{"id":"4lOJAHsojr27","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}